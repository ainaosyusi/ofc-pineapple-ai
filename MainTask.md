OFC Pineapple AI開発仕様書・計画書
1. プロジェクト概要
目的: JOPTルールに準拠したOpen-Face Chinese Poker (Pineapple) の超人的AIエージェントの開発。

最終目標: SaaS化を見据えたAPI化、および多人数戦における「相手の公開情報（アウト）」を考慮した最適戦略の導出。

技術的要件:

コアロジックの高速化（C++実装 + Pythonラッパー）。

深層強化学習（Deep RL）による戦略獲得。

AWSを用いた24時間継続学習環境の構築。

2. 技術スタック選定 (Performance & Scalability)
学習速度（FPS: Frames Per Second）がAIの強さに直結するため、計算コストの高い部分はコンパイル言語を使用します。

ゲームエンジン (Core Logic): C++ (C++17以上)

理由: ハンド判定、点数計算、カード配布シミュレーションを最速で行うため。

データ構造: ビットボード（Bitboard）を活用し、論理演算で高速に判定。

インターフェース: pybind11

C++の関数をPythonモジュールとして直接呼び出すためのバインディングツール。

学習制御・モデル: Python + PyTorch

強化学習アルゴリズムの実装。

RLフレームワーク: Ray RLLib (または Stable Baselines3)

将来的にAWSでの分散学習（複数のCPUでゲームを回し、GPUで学習）を行うため、分散処理に強い Ray の採用を推奨します。

インフラ: AWS (EC2 Spot Instances + S3), Docker

3. ゲームシステム仕様 (JOPT準拠 / OFC Pineapple)
ルールエンジンの正確性が最優先です。以下の仕様をC++で実装します。

3.1 基本ルール

デッキ: 52枚（初期）、52枚 + Joker 2枚（フェーズ2で実装）

プレイヤー数: 2〜3人

構成:

Top (上段): 3枚

Mid (中段): 5枚

Bottom (下段): 5枚

役の強さ: Bottom >= Mid >= Top でなければならない（満たさない場合はFoul/ファウル）。

3.2 ゲーム進行 (Pineapple)

Start: 各プレイヤーに5枚配布。

Set 1: 5枚すべてをTop/Mid/Botのいずれかに配置（配置後の移動不可）。

Turn 2〜5:

各プレイヤーに 3枚 配布。

2枚 を配置し、1枚 を裏向きで捨てる（Discard）。

これを4回繰り返し、計13枚を配置して完了。

Showdown: 点数計算。

3.3 点数計算ロジック (Scoring)

ロイヤリティ (Royalties): 役に応じたボーナス点（JOPT配点表をハードコード）。

勝敗判定: 段ごとに勝負（Top vs Top, Mid vs Mid, Bot vs Bot）。

勝ち: +1点

負け: 0点（引き算で-1点扱いにすることが多い）

スクープ (Scoop): 全段勝利で +3点のボーナス。

ファウル (Foul): 相手に6点献上（+相手のロイヤリティ）。

3.4 ファンタジーランド (Fantasy Land / FL)

突入条件: TopがQQ以上（JOPT基準）で、かつファウルしていない。

FLの挙動:

次回、カードをまとめて 14枚 配布（Pineappleの場合）。

その中から13枚をセットし、1枚捨てる。

相手は通常通り進行。

FL継続: FL中にさらに強い役（Top Trips以上、Bot Quads以上など）を作れば継続。

4. AIアーキテクチャ設計 (Deep RL)
相手の公開カード（アウト）を認識させるため、ニューラルネットワークの入力設計が重要です。

4.1 入力層 (Observation Space)

盤面情報を「画像」のようなテンソルとして扱います。 サイズ例: (N, 52 + 2, Features)

自分のボード情報: 各ポジションにどのカードがあるか（One-hot vector）。

現在の手札: 今配られた3枚。

相手の公開ボード情報: 【重要】 ここを入れることで、「相手がAを2枚使っているから、自分はAを引く確率が低い」ことを学習させます。

見えている捨て札: デッキの残りを推論するため。

ゲーム状態: 何ターン目か、FL中か、ディーラーボタン位置。

4.2 出力層 (Action Space)

Pineapple特有の「3枚から2枚選び、場所を決める」行動。

Action: (Card1_Pos, Card2_Pos, Discard_Card_Index)

無効な手（埋まっている場所に置くなど）は、Action Masking技術で除外します。

4.3 学習アルゴリズム

PPO (Proximal Policy Optimization): 安定して学習できるため、ベースラインとして採用。

報酬設計 (Reward Shaping):

基本報酬: 獲得点数（+点/-点）。

補助報酬（初期のみ）: ファウル回避に正の報酬、強い役作りに微小な報酬。

5. 開発ロードマップ & マイルストーン
大学の講義や就活と並行できるよう、フェーズを分けます。

Phase 1: 高速ゲームエンジン開発 (C++)

目標: ルールを完璧に実装し、ランダムプレイヤー同士で1秒間に数万ゲームのシミュレーションができる状態。

ToDo:

Card, Deck, HandEvaluator クラスの実装。

GameEngine クラスの実装（JOPT点数計算含む）。

単体テスト作成（特定の配置で正しい点数が出るか厳密にチェック）。

pybind11でPythonから呼び出しテスト。

Phase 2: 学習環境構築 (Python/Local)

目標: AIが「ファウルを避ける」「基本的な役を作る」ようになる。

ToDo:

Gymnasium (OpenAI Gym) 互換の環境クラス作成。

PPOを用いた学習ループの実装。

まずは「対戦相手なし（ソリティア）」で最大得点を目指す学習を行う。

Phase 3: 対戦 & アウト読み学習 (Local -> AWS)

目標: 相手のカードを見て確率的に有利な手を打つ。

ToDo:

Multi-Agent学習の実装（AI vs AI）。

AWS EC2 (g4dn.xlarge等) での環境構築。

Dockerコンテナ化。

チェックポイント（モデル）の自動保存。

Phase 4: ルール拡張 (Joker導入)

目標: "最近流行りのルール"への対応。

ToDo:

デッキ枚数変更、Jokerのワイルドカード判定ロジック追加（ここが技術的に一番難しいです。Jokerは「その時点で最強のカード」として判定する必要があるため）。

分散（Variance）が大きくなるため、学習時間を増やす。

---------------------------------------------------------
OFC Pineapple AI 開発総合仕様書・計画書
1. プロジェクト概要
プロジェクト名: Project Pineapple AI (仮)

目的: JOPTルールおよび最新のトレンド（プログレッシブFL、Joker入り）に対応した、超人的なOFC AIの開発。

核心戦略:

C++による高速計算: 数億回のシミュレーションに耐えうる堅牢なエンジン。

深層強化学習 (Deep RL): 「アウツ（場に見えているカード）」を完全考慮した確率的最適解の導出。

スケーラビリティ: AWSを用いた24時間継続学習システム。

2. ゲームシステム仕様 (Rules & Mechanics)
実装言語: C++ (Core Logic) / Python (Wrapper)

2.1 基本設定

参加人数: 2〜3人（ヘッズアップ〜3MAX）

デッキ:

Phase 1: 52枚（通常デッキ）

Phase 2: 54枚（52枚 + Joker 2枚）

ゲーム進行 (Pineapple):

初期配布: 5枚配布 → 5枚配置。

ターン進行 (x4): 3枚配布 → 2枚配置 / 1枚ディスカード（裏向き）。

判定: 計13枚配置完了後、役判定と点数計算を行う。

2.2 点数計算 (Scoring Table)

以下の指定テーブルをハードコードします。

役 (Hand Rank)	下段 (Bottom)	中段 (Middle)	上段 (Top)
66	-	-	1点
77	-	-	2点
88	-	-	3点
99	-	-	4点
TT	-	-	5点
JJ	-	-	6点
QQ	-	-	7点
KK	-	-	8点
AA	-	-	9点
222	-	-	10点
333	-	-	11点
... (各+1点)	-	-	...
AAA	-	-	22点
3カード	-	2点	(上段参照)
ストレート	2点	4点	-
フラッシュ	4点	8点	-
フルハウス	6点	12点	-
4カード	10点	20点	-
ストフラ	15点	30点	-
ロイヤル	25点	50点	-
ファウル (Foul): 役の強さが Bottom >= Middle >= Top を満たさない場合。

ペナルティ: 全段負け扱い（-6点）+ 相手のロイヤリティ全額支払い + スクープボーナス献上。

2.3 ファンタジーランド (Fantasy Land) 仕様

プログレッシブFL（枚数変動型） を実装オプションとして採用します。

突入条件: ファウルせず、Topが QQ 以上であること。

配布枚数 (次回):

Top QQ: 14枚

Top KK: 15枚

Top AA: 16枚

Top Trips (222以上): 17枚

アクション: 配布された枚数から13枚をセットし、残りをディスカード。

継続条件 (Stay):

Top: 222 以上

Bottom: 4カード 以上

※継続時の配布枚数は一律14枚（またはルール設定により固定）とするが、まずは14枚固定で実装。

3. AI技術仕様 (Technical Architecture)
3.1 状態空間 (State Representation)

AIに入力する「盤面情報」の設計です。アウツ（残り枚数）を正確に把握させるため、以下の情報をTensor化します。

Input Tensor Shape: (Batch, Channels, 52) (カード52枚を1次元として扱う場合)

My Board (自分): Top/Mid/Bot の埋まっているカード (3 channels)

My Hand (現在の手札): 配られた3枚 (1 channel)

Opponent Boards (相手): 【重要】 相手の公開カード（アウツ計算用）。最大2名分 (2 channels)

Discards (捨て札):

自分の捨て札 (既知)

相手の捨て札 (未知だが、カウンティング済みならDead Cardとして扱うか、あるいは確率分布として扱う) ※基本は「見えているカード」を入力に含める。

Global Info: 現在のストリート(1-5)、FL中フラグ、ボタン位置。

3.2 コアエンジン (C++)

Pythonでの処理遅延を防ぐため、ゲームロジックはC++で記述し、共有ライブラリ(game_engine.so)としてコンパイルします。

Bitboard実装:

カード1枚を特定のビットに対応させ、uint64_t でハンドを表現。

フラッシュ判定やペア判定をビット演算(&, ^, popcount)で行い、高速化。

Hand Evaluator:

7枚ポーカーと異なり、OFCは3枚/5枚の判定頻度が極めて高い。OFC専用のルックアップテーブルを作成。

MCTS (モンテカルロ木探索):

推論時、Policy Networkの確率に加え、C++側で軽量シミュレーションを行い、補正をかける（オプション）。

3.3 学習インフラ (AWS)

構成:

Trainer Node (GPU): モデルの更新 (Gradient Descent) を担当。g4dn.xlarge (T4 GPU) 等を使用。

Worker Nodes (CPU): C++エンジンを動作させ、自分自身(Self-Play)と対戦し、経験データ(Replay Buffer)を作成。c6a.2xlarge 等の計算最適化インスタンスを使用。

フレームワーク: Ray (RLlib) または分散対応の独自スクリプト。

24時間稼働: Spot Instanceを活用し、コストを抑えつつ中断時は自動再開するスクリプトを組む。

4. 開発ロードマップ (Timeline)
Phase 1: コアエンジン開発 (Local / C++)

Goal: ルール通りにゲームが進行し、点数計算が100%正確に行われること。

Task:

Card, Deck 構造体の定義。

Evaluator (点数表の実装) の作成と単体テスト。

GameState クラスの実装 (配布、配置、FL判定)。

【検証】 ランダムに配置させ、手計算と一致するか100パターンのテストケースを通す。

Phase 2: 強化学習環境の構築 (Local / Python)

Goal: AIがルールを理解し、ファウル率を低下させる。

Task:

pybind11 で C++エンジンを Pythonモジュール化。

Gymnasium (OpenAI Gym) インターフェースの実装。

単純なPPO (Proximal Policy Optimization) で学習開始。

アウツを考慮せず「自分の手役」だけで最大化するAIを作成（ベースライン）。

Phase 3: アウツ考慮 & 対戦学習 (Local -> AWS)

Goal: 相手のオープンカードを見て、危険牌や有効牌を判断できる。

Task:

Observation Spaceに「相手のボード情報」を追加。

Self-Play (自分 vs 自分) のループを構築。

AWS EC2 インスタンスのセットアップとデプロイ。

Phase 4: ルール拡張 (Joker & Variance)

Goal: Joker入りルールへの対応。

Task:

Jokerのワイルドカード論理（そのハンドで最強になるカードに変化）の実装。

分散拡大に伴う、学習エポック数の増加。