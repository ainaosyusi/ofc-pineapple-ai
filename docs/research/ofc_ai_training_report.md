# OFC Pineapple AI 強化学習研究レポート

**プロジェクト:** Open-Face Chinese Poker Pineapple AI  
**研究者:** [著者名]  
**日付:** 2026年1月15日  
**バージョン:** 1.0

---

## 概要

本研究は、Open-Face Chinese Poker Pineapple（OFC Pineapple）における深層強化学習エージェントの開発を目的とする。OFC Pineappleは不完全情報ゲームであり、長期的な計画と確率計算が要求される複雑なドメインである。

---

## 1. 問題設定

### 1.1 ゲーム概要

OFC Pineappleは、52枚のデッキから配られたカードを3つの列（Top: 3枚、Middle: 5枚、Bottom: 5枚）に配置し、役の強さを競う2-3人用カードゲームである。

**主なルール制約:**
- Bottom ≥ Middle ≥ Top の役強度順序を維持（違反=「ファウル」）
- 一度配置したカードは移動不可
- 13ターンで完成

### 1.2 研究目標

1. 自己対戦（Self-Play）による学習エージェントの構築
2. ファウル回避と役作り戦略の獲得
3. Fantasy Land突入戦略の最適化

---

## 2. 技術アーキテクチャ

### 2.1 システム構成

```
┌─────────────────────────────────────────────────┐
│           OFC Pineapple AI System               │
├─────────────────────────────────────────────────┤
│                                                 │
│  ┌─────────────┐      ┌─────────────────────┐  │
│  │   C++ Engine │◄────►│   Python Wrapper    │  │
│  │  (pybind11)  │      │   (Gymnasium)       │  │
│  └─────────────┘      └─────────────────────┘  │
│         │                       │               │
│         ▼                       ▼               │
│  ┌─────────────┐      ┌─────────────────────┐  │
│  │ Bitboard    │      │  Multi-Agent Env    │  │
│  │ Evaluator   │      │  (PettingZoo)       │  │
│  └─────────────┘      └─────────────────────┘  │
│                                 │               │
│                                 ▼               │
│                       ┌─────────────────────┐  │
│                       │    PPO / MaskablePPO │  │
│                       │  (Stable-Baselines3) │  │
│                       └─────────────────────┘  │
│                                                 │
└─────────────────────────────────────────────────┘
```

### 2.2 観測空間

| 特徴量 | 形状 | 説明 |
|-------|------|------|
| my_board | MultiBinary(156) | 自分のボード (3行 × 52カード) |
| opponent_board | MultiBinary(156) | 相手の公開ボード |
| hand | MultiBinary(260) | 現在の手札 (最大5枚 × 52) |
| used_cards | MultiBinary(52) | 使用済みカード |
| game_state | Box(8) | ストリート、各行カード数等 |
| **probabilities** | **Box(8)** | **フラッシュ/ストレート完成確率 (自分/相手)** |

### 2.3 行動空間

- **初期配置 (Street 1):** 5枚のカードを3行に配置 → 組み合わせ数: 243通り
- **通常ターン:** 3枚から2枚選択、1枚捨て → 行動ID: 243通り

---

## 3. 実験と結果

### 3.1 実験1: 初期Self-Play学習

**設定:**
- アルゴリズム: PPO (Proximal Policy Optimization)
- 総ステップ数: 500,000
- 対戦相手更新頻度: 50,000ステップ

**結果:**

| 指標 | 値 |
|------|-----|
| 総ゲーム数 | 約3,400 |
| ファウル率 | **87.5%** |
| 勝率 | 0% |
| 学習時間 | 約9分 (EC2 m7i-flex.large) |

**考察:**

ファウル率87.5%は、エージェントが「役の強度順序」というゲームの基本ルールを学習できていないことを示す。これは強化学習における典型的な**「死の谷」問題**である。

### 3.2 実験2: Action Masking とカリキュラム学習 (Phase 1)

**設定:**
- アルゴリズム: **MaskablePPO**
- 報酬設計: Phase 1 (ファウル回避特化: Valid +10 / Foul -10)
- 総ステップ数: 100,000
- アクション選択: **有効アクションのみに限定 (Action Masking)**

**結果:**

| 指標 | 初期実験 (PPO) | 改善実験 (MaskablePPO) |
|------|----------------|------------------------|
| ファウル率 | 87.5% | **77.0%** |
| Success数 | 0 / 8 | **23 / 100** |
| 学習時間 | 約9分 (500k) | **約1分 (100k)** |
| FPS | ~1,100 | **~1,600** |

**考察:**

1. **学習効率の飛躍的向上:** 
   MaskablePPOの導入により、探索空間が物理的に不可能なアクションを除去したことで、収束速度が大幅に向上した。わずか10万ステップの学習で、以前の50万ステップよりも優れたファウル回避能力を獲得した。

2. **基本構造の理解:** 
   Successとして13枚並べられたゲームの盤面を確認すると、エージェントは「下段・中段に強いカードを置く」という構造を断片的に理解し始めている。ただし、依然としてファウル率が77%と高いため、さらに学習ステップ数を増やす必要がある。

3. **計算性能:** 
   ローカル環境（M1 Pro）において、C++エンジンとの連携により1,600 FPS以上の高速な学習を実現した。これにより、数百万ステップの学習も現実的な時間（1時間以内）で完了可能である。

### 3.3 学習フェーズの詳細定義とベンチマーク (完了)

本プロジェクトでは、段階的な学習（カリキュラム学習）を採用している。各フェーズの定義と最新の評価結果を以下にまとめる。

#### **Phase 1: 基礎ルールとファウル回避 (Solo)**
- **目的**: 3段のボード構造（3-5-5）と「役の強さの順位を守る」という基本ルールの学習。
- **環境**: シングルプレイヤー。
- **報酬設計**: 有効な配置 +10 / ファウル -10。
- **到達度**: ファウル率 約38%。基礎的な役の並べ方は習得済み。

#### **Phase 2: 役作りとロイヤリティの最適化 (Solo)**
- **目的**: 有効な配置を維持しつつ、高いロイヤリティ（ボーナス点）を狙う戦略の獲得。
- **環境**: シングルプレイヤー。
- **報酬設計**: 有効な配置 0 / ファウル -30 + ロイヤリティボーナス。
- **到達度**: **ファウル率 約32%（全フェーズで最小）**。最も安定したバランスを実現。

#### **Phase 3: 自己対戦による実戦戦略 (Multi-Agent)**
- **目的**: 相手の公開カードを考慮した期待値の最大化とリスク管理。
- **環境**: 2人対戦（Self-Play）。
- **報酬設計**: 相対的な勝敗スコア + ロイヤリティ。
- **特徴**: 
    - **Standard**: 基本的な対戦。
    - **Enhanced**: 自分の完成確率だけでなく、**「相手に引かれたくないカード」を考慮する確率特徴量**を追加。
- **到達度**: ファウル率は 58-63% と高め。これはランダムな相手に対しても「相手を上回る役」を狙いに行くアグレッシブな戦略（リスクテイク）が学習されているためと考えられる。

### 3.4 評価指標の比較 (2026/01/16 最新)

500ゲームのテストプレイに基づく全フェーズモデルの比較。

| フェーズ | モデル名 | ファウル率 | 平均ロイヤリティ | 平均報酬 | 評価 |
| :--- | :--- | :---: | :---: | :---: | :--- |
| **Phase 1** | `latest_phase1_model.zip` | 37.8% | 0.34 | -11.33 | 基礎習得 |
| **Phase 2** | `ofc_phase2...final.zip` | **32.0%** | 0.26 | **-9.59** | **最も安定** |
| **Phase 3 (Std)** | `ofc_phase3...final.zip` | 63.0% | 0.00 | -14.91 | 攻撃的 |
| **Phase 3 (Enh)** | `enhanced_ppo_final.zip` | 58.0% | 0.00 | -13.54 | 確率考慮 |

### 3.5 学習曲線 (Phase 1)
初期学習におけるファウル率の低下推移。
![Phase 1 Learning Curve](/Users/naoai/.gemini/antigravity/brain/1fa0ab1e-94cc-489c-98bc-ccb6a0f89a3f/learning_curve_phase1.png)

### 3.5 実験5: 強化版 AI (確率特徴量 & MCTS 統合)

**新機能:**
1. **確率計算エンジン**: C++でフラッシュ/ストレートの完成確率をリアルタイム計算し、観測空間に追加。
2. **Policy-guided Rollout**: MCTSのシミュレーションにおいて、ランダムではなく学習済みPolicyを用いて先読みを行う。
3. **Endgame Solver**: 残り5枚以下の局面を完全解析し、100%の最適解を出力。

### 3.6 実験6: Auto-Curriculum & Feedback システム

学習進捗をAIが自己評価し、Discordを通じて人間に評価とフィードバックを送るシステムを構築。

### 3.7 実験7: Phase 4 ジョーカー対応 (Joker Integration)

**目的:** JOPTルールに準拠した54カード環境（Joker 2枚を含む）での学習。

**技術的実装:**

1. **C++エンジン拡張:**
   - `card.hpp`: `NUM_CARDS` を52から54に拡張、`JOKER_MASK` を定義
   - `evaluator.hpp`: ジョーカーをワイルドカードとして扱うO(1)ヒューリスティック判定を実装
   - 総当たり計算を排し、「ジョーカーが何枚あればこの役が成立するか」を条件分岐で判定

2. **Python環境更新:**
   - `ofc_phase1_env.py`, `ofc_env.py`, `multi_ofc_env.py` の観測空間を54カード対応に拡張

**意地悪テスト結果:**
| テストケース | 期待値 | 結果 |
|:---|:---|:---:|
| As + JK + JK | THREE_OF_A_KIND (A) | ✅ |
| 2s-5s + JK | STRAIGHT_FLUSH (Wheel) | ✅ |
| 2s, 2h, 3d, 4c, JK | THREE_OF_A_KIND | ✅ |
| 8s, 8h, 8d, 9c, JK | FOUR_OF_A_KIND | ✅ |
| As, Ks, Qs, Ts, JK | ROYAL_FLUSH | ✅ |
| Mid(QQQQ) > Top(KKK) | Valid (Not a Foul) | ✅ |

**ローカルテスト結果（10万ステップ）:**
| ステップ | ファウル率 | Mean Royalty |
|:---|:---:|:---:|
| 開始時 | 69% | 0.1 |
| 終了時 | 61% | 0.1 |

**EC2長期学習（進行中）:**
- インスタンス: `ofc-training-enhanced` (18.182.32.72)
- 目標ステップ: 10,000,000
- FPS: 約600
- 推定完了時間: 約4.6時間

**最終評価結果（1050万ステップ、1000ゲーム）:**
| 指標 | 結果 |
|:---|:---:|
| **ファウル率** | **25.10%** |
| **平均ロイヤリティ** | **0.85** |
| **Fantasyland突入率** | **1.10%** |

### 3.8 フェーズ間比較分析

| フェーズ | モデル名 | ファウル率 | 平均ロイヤリティ | FL率 | 評価 |
| :--- | :--- | :---: | :---: | :---: | :--- |
| **Phase 1** | `latest_phase1_model.zip` | 37.8% | 0.34 | - | 基礎習得 |
| **Phase 2** | `ofc_phase2...final.zip` | 32.0% | 0.26 | - | 安定 |
| **Phase 3 (Std)** | `ofc_phase3...final.zip` | 63.0% | 0.00 | - | 攻撃的 |
| **Phase 3 (Enh)** | `enhanced_ppo_final.zip` | 58.0% | 0.00 | - | 確率考慮 |
| **Phase 4 (Joker)** | `10500000_steps.zip` | **25.10%** | **0.85** | **1.10%** | **🏆 最良** |

**分析:**
- Phase 4は**全フェーズ中最低のファウル率（25.10%）**を達成
- 平均ロイヤリティ（0.85）もPhase 1（0.34）、Phase 2（0.26）を大幅に上回る
- ジョーカーは「困ったら役を完成させる保険」として効果的に機能
- Fantasyland突入率1.10%は改善の余地があるが、ファウル回避優先の戦略が反映されている

### 3.9 実験9: Phase 5 3-Max Self-Play (3人対戦)

**目的:** 2人対戦から3人対戦（3-Max）への拡張。ポジション戦略、アウツ計算の学習。

**技術的実装:**

1. **環境拡張 (ofc_3max_env.py):**
   - PettingZoo AECEnv準拠の3人マルチエージェント環境
   - プレイヤーID: `player_0`, `player_1`, `player_2`
   - ボタン位置による行動順序の動的管理

2. **観測空間の拡張（7チャンネル、約720次元）:**

| チャンネル | サイズ | 説明 |
|:---|---:|:---|
| my_board | 162 | 自分のボード |
| my_hand | 270 | 手札（最大5枚） |
| next_opponent_board | 162 | 下家の公開ボード |
| prev_opponent_board | 162 | 上家の公開ボード |
| my_discards | 54 | 自分の捨て札履歴 |
| unseen_probability | 54 | 確率マップ（見えない札の残存確率） |
| position_info | 3 | ポジション（BTN=0, SB=1, BB=2） |

> **重要:** 相手の捨て札は観測に含めない。これにより「完全情報ではない」実戦的な状況を再現。

3. **スコアリング（Triangle Scoring）:**
   ```
   A vs B: ±点数
   A vs C: ±点数
   B vs C: ±点数
   
   Aの収支 = (A vs B) + (A vs C)
   ※ファウル時は2人に支払い（×2のダメージ）
   ```

4. **Self-Play対戦相手選択:**
   - 最新モデル: 80%
   - 過去モデルプール: 20%

**学習目標:**

| 能力 | 説明 |
|:---|:---|
| アウツ・ブロック | 相手が集めているスートを避ける |
| ポジション戦略 | 後出し有利を活用した安全策選択 |
| リスク管理 | 3人戦での慎重なファウル回避 |

**AWS学習設定（進行中）:**
- インスタンス: `i-01a9f4e8ccbb49602` (52.68.185.12)
- 目標ステップ: 20,000,000
- FPS: 約378
- 推定完了時間: 約14.7時間
- 開始日時: 2026-01-16 13:47 JST

---



## 4. 問題分析

### 4.1 信用割当問題 (Credit Assignment Problem)

OFC Pineappleは13ターンで構成されるが、報酬はゲーム終了時のみ与えられる。ファウルは最終的な結果であり、どのターンの意思決定が原因かを特定することが困難。

```
Turn 1 ─► Turn 2 ─► ... ─► Turn 13 ─► [ファウル!]
                                            │
                            どのターンが原因？
```

### 4.2 スパース報酬

現在の報酬設計:
```python
# 問題のある報酬設計
if won:
    reward = +score  # 例: +15
else:
    reward = -score  # 例: -15
```

13ターン × 約145ステップの後にしか報酬が得られないため、学習シグナルが極めて薄い。

### 4.3 無効アクションの選択

現在のPPOは243個のアクションすべてを探索対象としているが、実際に有効なアクションは状況により大幅に制限される。無効アクションに対する探索は学習効率を著しく低下させる。

---

## 5. 改善提案

### 5.1 Action Masking（最優先）

**手法:** sb3-contribのMaskablePPOを導入し、有効なアクションのみを選択可能にする。

```python
from sb3_contrib import MaskablePPO

def valid_action_mask(self) -> np.ndarray:
    """有効なアクションのマスクを返す"""
    mask = np.zeros(243, dtype=bool)
    for action_id in self.get_valid_actions():
        mask[action_id] = True
    return mask
```

**期待効果:** 探索空間の削減により学習効率が数倍向上

### 5.2 カリキュラム学習

段階的にタスク難易度を上げることで、基礎から高度な戦略まで効率的に学習。

```mermaid
graph LR
    P1[Phase 1: ファウル回避] --> P2[Phase 2: 役作り]
    P2 --> P3[Phase 3: 対戦戦略]
    
    P1 -.- R1["報酬: ±10"]
    P2 -.- R2["報酬: ロイヤリティ"]
    P3 -.- R3["報酬: JOPTスコア"]
```

**Phase 1 報酬関数:**
```python
def calculate_reward_phase1(self):
    if not self.is_game_over():
        return 0.0  # 途中報酬なし
    
    if self.is_valid_board():
        royalties = self.get_royalties()
        return 10.0 + (royalties * 0.1)
    else:
        return -10.0  # ファウル
```

### 5.3 ポテンシャルベース報酬シェーピング（将来検討）

中間報酬として「役の完成度」を与えることも検討可能だが、**Reward Hacking**のリスクがある。

```
⚠️ リスク例:
「フラッシュドロー状態」に報酬を与えると、
AIがドローを維持し続けて役を完成させない行動を学習する可能性。
```

---

## 6. 実装計画

### 6.1 Phase 1 実装項目

### 6.1 フェーズ別達成状況

| 項目 | 優先度 | 状態 | 備考 |
| :--- | :---: | :---: | :--- |
| **Phase 1: 基礎ルール学習** | 🔴 最高 | **完了** | MaskablePPO導入、ファウル率改善 |
| **Phase 2: スコア最適化** | 🔴 最高 | **完了** | ロイヤリティ報酬の統合 |
| **Phase 3: 対戦戦略 (Self-Play)** | � 高 | **完了** | EC2での大規模学習 (1,000万歩) |
| **Phase 4: ジョーカー (Joker) 対応** | 🔴 最高 | **未着手** | **次期主要タスク** |
| **Phase 5: 実戦（JOPTルール）評価** | � 中 | 未着手 | 最終評価フェーズ |

### 6.2 Phase 4 (Joker対応) 実装計画

1. **C++エンジンの拡張**: 
   - 52枚 + Joker 2枚へのデッキ拡張。
   - Jokerを任意のカードとして扱う役判定ロジックの最適化。
2. **観測空間の更新**:
   - Joker保持情報の追加。
3. **報酬の再定義**:
   - Jokerによる高得点（大役）の発生頻度増加に合わせたスケーリング。

### 6.2 成功基準

- **Phase 1目標:** ファウル率 < 10%
- **Phase 2目標:** 平均ロイヤリティ > 3点
- **Phase 3目標:** ランダムエージェントに90%以上勝利
- **Phase 4目標:** ジョーカー（Joker）を含む実戦環境への対応

---

## 7. 関連研究

- **AlphaGo / AlphaZero:** モンテカルロ木探索 + 深層強化学習
- **Poker AI (Libratus, Pluribus):** 不完全情報ゲームにおけるCFR
- **Action Space Shaping:** 大規模離散行動空間の効率的学習

---

## 8. 結論

OFC Pineapple AIの初期実験では、高いファウル率という課題が明らかになった。これはスパース報酬と信用割当問題に起因する。**Action Masking**と**カリキュラム学習**の導入により、段階的にこの問題を解決し、実用的なAIエージェントの構築を目指す。

---

## 付録

### A. 実験環境

| 項目 | 仕様 |
|------|------|
| ローカル | macOS, Python 3.9, M1 Pro |
| クラウド | AWS EC2 m7i-flex.large (2 vCPU, 8GB) |
| ライブラリ | Stable-Baselines3 2.7.1, PettingZoo 1.25.0 |
| C++コンパイラ | g++ 14.2.0 (C++17) |

### B. ソースコード構成

```
OFC NN/
├── src/cpp/           # C++ゲームエンジン
│   ├── game.hpp       # メインエンジン
│   ├── evaluator.hpp  # 役評価
│   └── pybind/        # Python バインディング
├── src/python/        # Python学習コード
│   ├── multi_ofc_env.py  # マルチエージェント環境
│   ├── train_selfplay.py # Self-Play学習
│   └── train_aws.py      # AWS対応版
└── docs/research/     # 研究レポート
```

---

*本レポートは継続的に更新される。*
