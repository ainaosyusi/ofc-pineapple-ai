# Phase 8 Multi-Variant Training Analysis Report

**作成日**: 2026-01-18
**分析対象期間**: Phase 8開始 ～ 現在

---

## 1. Executive Summary

Phase 8では3つの学習バリアント（Self-Play, Aggressive, Teacher）をGCPで並行学習した。
約9.8M～6.8Mステップの学習を経て、以下の結論に至った：

| 結論 | 詳細 |
|:---|:---|
| **学習は停滞（プラトー）に達した** | ファウル率は20-30%で振動し、一方向への改善は見られない |
| **最良記録** | Self-Play: 20.8% (Step 4.6M), Aggressive: 21.4% (Step 3.7M) |
| **推奨アクション** | 純粋RLの限界。Hybrid Agent（RL + Solver）への移行を推奨 |

---

## 2. Training Configuration

### 2.1 インスタンス構成

| バリアント | GCP Instance | IP | 特徴 |
|:---|:---|:---|:---|
| **Self-Play** | ofc-training | 35.243.93.32 | 過去モデルとの対戦で自己改善 |
| **Aggressive** | ofc-aggressive | 34.146.34.141 | FL報酬+25, ロイヤリティ×1.5の報酬シェーピング |
| **Teacher** | ofc-teacher | 35.200.57.236 | ルールベースエージェントからの模倣学習 |

### 2.2 共通パラメータ

```
Algorithm: MaskablePPO
Learning Rate: 0.0001
Batch Size: 8192
Parallel Envs: 4
Total Target Steps: 20,000,000
```

---

## 3. Training Results

### 3.1 Self-Play Variant

**総ステップ数**: 9,800,000 (49% complete)

#### 学習曲線

| Step | Foul Rate | Mean Score | Mean Royalty | FL Entry |
|---:|---:|---:|---:|---:|
| 100K | 27.0% | +7.20 | 1.11 | 1.4% |
| 1M | 23.2% | +7.39 | 1.08 | 3.0% |
| 2M | 26.0% | +7.27 | 1.12 | 1.4% |
| 3M | 25.4% | +7.07 | 1.00 | 1.8% |
| 4M | 26.4% | +6.96 | 1.09 | 1.8% |
| **4.6M** | **20.8%** | +7.67 | 1.06 | 2.0% |
| 5M | 26.0% | +7.14 | 1.14 | 2.0% |
| 6M | 23.0% | +7.56 | 1.12 | 1.8% |
| 7M | 27.0% | +6.66 | 0.96 | 2.0% |
| 8M | 26.4% | +6.88 | 0.94 | 2.0% |
| 9M | 26.6% | +6.68 | 1.06 | 2.0% |
| **9.7M** | **22.2%** | +7.87 | 1.14 | 3.2% |

**ベストパフォーマンス**: Step 4,600,000 - Foul Rate 20.8%

#### 学習安定性指標

- **Explained Variance**: 0.17-0.22 (低い - 価値関数の予測精度が不十分)
- **Entropy Loss**: -0.21 (ポリシーの探索性は維持)
- **Approx KL**: 0.022 (適正範囲)

---

### 3.2 Aggressive Variant

**総ステップ数**: 6,800,000 (34% complete)

#### 学習曲線

| Step | Foul Rate | Mean Score | Mean Royalty | FL Entry |
|---:|---:|---:|---:|---:|
| 100K | 24.0% | +7.48 | 1.32 | 2.0% |
| 1M | 29.0% | +6.59 | 1.02 | 1.6% |
| 2M | 28.4% | +6.51 | 0.89 | 1.4% |
| 3M | 26.0% | +6.67 | 0.96 | 2.2% |
| **3.7M** | **21.4%** | +7.81 | 1.15 | 2.6% |
| 4M | 26.2% | +6.96 | 1.23 | 1.2% |
| 5M | 26.8% | +6.85 | 1.05 | 1.6% |
| 6M | 23.0% | +7.55 | 1.14 | 3.0% |
| 6.8M | 25.4% | +7.27 | 1.13 | 2.0% |

**ベストパフォーマンス**: Step 3,700,000 - Foul Rate 21.4%

#### 特徴

- FL突入率が最大3.2%と他バリアントより高い傾向
- ファウル率の分散が大きい（21.4% - 29.8%）
- 報酬シェーピングの効果は限定的

---

### 3.3 Teacher Learning Variant

**総ステップ数**: 6,800,000 (34% complete)

#### 学習曲線

| Step | Foul Rate | Mean Score | Mean Royalty | FL Entry |
|---:|---:|---:|---:|---:|
| 100K | 27.0% | +6.11 | 0.84 | 1.6% |
| 1M | 27.0% | +6.43 | 1.08 | 2.0% |
| 2M | 27.4% | +6.31 | 0.89 | 2.0% |
| 3M | 26.2% | +6.79 | 1.05 | 2.2% |
| 4M | 25.2% | +6.53 | 1.02 | 2.0% |
| 5M | 26.4% | +6.84 | 1.05 | 1.6% |
| 6M | 26.0% | +6.68 | 0.91 | 3.2% |
| **6.3M** | **24.8%** | +7.58 | 1.09 | 2.0% |
| 6.8M | 29.4% | +6.56 | 0.96 | 1.4% |

**ベストパフォーマンス**: Step 6,300,000 - Foul Rate 24.8%

#### 特徴

- 3バリアント中、最も安定した学習曲線
- しかし、最良記録は他バリアントに劣る
- KL発散が0.04と高く、学習が不安定になりつつある

---

## 4. Comparative Analysis

### 4.1 Phase 7 vs Phase 8 比較

| 指標 | Phase 7 | Phase 8 Self-Play | Phase 8 Aggressive | Phase 8 Teacher |
|:---|---:|---:|---:|---:|
| **Foul Rate (Best)** | 25.8% | **20.8%** | 21.4% | 24.8% |
| **Foul Rate (Avg)** | - | 25.1% | 26.0% | 26.8% |
| **Mean Score** | +7.56 | +7.20 | +7.10 | +6.80 |
| **FL Entry Rate** | 1.1% | 2.0% | 2.2% | 1.8% |
| **Total Steps** | 20M | 9.8M | 6.8M | 6.8M |

### 4.2 バリアント間比較

```
ファウル率（最良）:
  Self-Play:  ████████████████████░░░░░░░░░░ 20.8%
  Aggressive: █████████████████████░░░░░░░░░ 21.4%
  Teacher:    ████████████████████████░░░░░░ 24.8%

FL突入率（平均）:
  Self-Play:  ██░░░░░░░░ 2.0%
  Aggressive: ██░░░░░░░░ 2.2%
  Teacher:    █░░░░░░░░░ 1.8%
```

---

## 5. Why Training Plateaued

### 5.1 技術的分析

1. **低いExplained Variance (0.15-0.22)**
   - Value Networkが実際のリターン分散の20%程度しか説明できていない
   - OFCはドロー運によるスコア分散が極めて大きく、NNだけでは期待値予測が困難

2. **高いスコア分散**
   - 同一ポリシーでも評価間で±5%のファウル率変動
   - 確率的なゲーム特性がシグナルをノイズに埋没させている

3. **報酬シェーピングの限界**
   - Aggressiveバリアントの報酬シェーピングは、FL突入率を僅かに改善したが
   - ファウル率の根本的改善には寄与しなかった

### 5.2 理論的限界

```
┌─────────────────────────────────────────────────────────┐
│  OFCの期待値 = f(配置戦略) + ε(ドロー運)                │
│                                                         │
│  NNが学習できるのは f(配置戦略) の部分のみ              │
│  ε(ドロー運) は本質的に予測不可能                       │
│                                                         │
│  → Explained Variance ≈ Var(f) / (Var(f) + Var(ε))     │
│  → OFCでは Var(ε) >> Var(f) なので、EV は低くなる      │
└─────────────────────────────────────────────────────────┘
```

---

## 6. Recommendations

### 6.1 即時アクション

| 優先度 | アクション | 期待効果 |
|:---|:---|:---|
| **高** | 最良チェックポイントの保存 | Self-Play 4.6M, Aggressive 3.7M |
| **高** | 学習の停止 | GCPコスト削減 |
| **中** | Hybrid Agent実装 | ファウル率 15%以下を目指す |

### 6.2 Hybrid Agent戦略

```
Round 1 (初手5枚):   RL Policy Network     → 大局観・定石
Round 2-3 (ドロー):  RL + 軽量MCTS         → リスクバランス
Round 4 (ドロー):    MCTS (探索増加)       → 確定的リスク回避
Round 5 (最終):      完全解析ソルバー       → ミスゼロの確定
```

### 6.3 期待される改善

| コンポーネント | 単体効果 | 累積効果 |
|:---|:---|:---|
| 最良RLモデル | 20.8% | 20.8% |
| + Round 5 Solver | -5% | ~15-16% |
| + MCTS補正 | -2% | ~13-14% |
| **目標** | - | **< 15%** |

---

## 7. Saved Checkpoints

### 7.1 利用可能なモデル

| Instance | Model Path | Steps | Note |
|:---|:---|---:|:---|
| Self-Play | `p8_selfplay_9800000.zip` | 9.8M | Latest |
| Self-Play | `p8_selfplay_5000000.zip` | 5M | Near best |
| Self-Play | `p8_selfplay_4000000.zip` | 4M | - |
| Aggressive | `aggressive_1000000.zip` | 1M | Early |
| Aggressive | `aggressive_800000.zip` | 800K | - |

### 7.2 推奨ベースモデル

1. **Self-Play 4.6M付近** - ファウル率20.8%の最良記録
2. **Aggressive 3.7M付近** - ファウル率21.4%、FL率2.6%

---

## 8. Conclusion

Phase 8のMulti-Variant学習は、純粋な強化学習の限界を明確に示した。
3つのバリアント全てが約25%前後のファウル率で停滞し、これ以上の学習時間投入は費用対効果が低い。

**次のフェーズ**: Hybrid Agent（RL + Solver）の構築により、推論時の計算力で性能を補完することで、Diamond Tier（ファウル率15%以下）達成を目指す。

---

*Report generated: 2026-01-18*
