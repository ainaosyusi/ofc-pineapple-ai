# Phase 8: Multi-Variant Training

**作成日**: 2026-01-18
**ステータス**: 完了（学習プラトー到達）

---

## 概要

Phase 8では、3つの異なる学習バリアントをGCP上で並行実行し、最適な学習戦略を探索した。

---

## 1. バリアント設計

### 1.1 Self-Play Variant

```
目的: 自己対戦による戦略の自己改善
報酬: 標準スコア + FL +15
特徴: 過去モデルとの対戦（latest 80%, past 20%）
```

**Self-Play Manager**:
```python
class SelfPlayOpponentManager:
    def __init__(self, pool_size=5, latest_prob=0.8):
        self.model_pool = []  # 過去モデル保持
        self.pool_update_freq = 200_000  # 更新頻度
```

### 1.2 Aggressive Variant

```
目的: FL突入を重視した攻撃的戦略の獲得
報酬: FL +25, Royalty ×1.5
特徴: 高リスク高リターンの行動を強化
```

**報酬シェーピング**:
```python
reward = base_score
if entered_fl:
    reward += 25  # FL突入ボーナス（通常+15を+25に）
reward += royalty * 1.5  # ロイヤリティを1.5倍評価
```

### 1.3 Teacher Learning Variant

```
目的: ルールベースエージェントからの知識移転
報酬: スコア + 教師一致ボーナス
特徴: 序盤は教師模倣、後半は独自探索
```

**教師エージェント**:
- ファウル回避を最優先
- 高位カードをTopに配置
- フラッシュ/ストレートの確率計算

---

## 2. 学習結果

### 2.1 Self-Play Variant

| 指標 | 初期 (100K) | 中期 (5M) | 最終 (9.8M) | ベスト |
|:---|---:|---:|---:|---:|
| **総ステップ** | 100K | 5M | 9.8M | 4.6M |
| **ファウル率** | 27.0% | 26.0% | 27.6% | **20.8%** |
| **Mean Score** | +7.20 | +7.14 | +6.90 | +7.67 |
| **FL Entry** | 1.4% | 2.0% | 2.0% | 2.0% |

**学習曲線の特徴**:
- 4-5Mステップ付近でファウル率20.8%の最良記録
- その後は25-28%で振動（改善せず）
- Explained Variance: 0.17-0.22（低い）

### 2.2 Aggressive Variant

| 指標 | 初期 (100K) | 中期 (3M) | 最終 (6.8M) | ベスト |
|:---|---:|---:|---:|---:|
| **総ステップ** | 100K | 3M | 6.8M | 3.7M |
| **ファウル率** | 24.0% | 26.0% | 25.4% | **21.4%** |
| **Mean Score** | +7.48 | +6.67 | +7.27 | +7.81 |
| **FL Entry** | 2.0% | 2.2% | 2.0% | 2.6% |

**学習曲線の特徴**:
- 報酬シェーピングによりFL突入率は若干向上（最大3.2%）
- ファウル率の分散が大きい（21.4% - 29.8%）
- 攻撃的すぎてファウルが増加する傾向

### 2.3 Teacher Learning Variant

| 指標 | 初期 (100K) | 中期 (3M) | 最終 (6.8M) | ベスト |
|:---|---:|---:|---:|---:|
| **総ステップ** | 100K | 3M | 6.8M | 6.3M |
| **ファウル率** | 27.0% | 26.2% | 29.4% | **24.8%** |
| **Mean Score** | +6.11 | +6.79 | +6.56 | +7.58 |
| **FL Entry** | 1.6% | 2.2% | 1.4% | 2.0% |

**学習曲線の特徴**:
- 最も安定した学習曲線
- しかし最良記録は他バリアントに劣る
- 教師模倣が探索を制限した可能性

---

## 3. 比較分析

### 3.1 バリアント間比較

```
ファウル率（ベスト）:
  Self-Play:   ████████████████████░░░░░░░░░░ 20.8% ← 最良
  Aggressive:  █████████████████████░░░░░░░░░ 21.4%
  Teacher:     ████████████████████████░░░░░░ 24.8%

FL突入率（最大）:
  Aggressive:  ███░░░░░░░ 3.2% ← 最高
  Self-Play:   ███░░░░░░░ 3.0%
  Teacher:     ██░░░░░░░░ 2.0%

学習安定性:
  Teacher:     ████████░░ 安定
  Self-Play:   ██████░░░░ 中程度
  Aggressive:  ████░░░░░░ 不安定
```

### 3.2 Phase 7 vs Phase 8

| 指標 | Phase 7 | Phase 8 (Best) | 変化 |
|:---|---:|---:|:---|
| ファウル率 | 25.8% | 20.8% | **-5.0%** ⬆️ |
| Mean Score | +7.56 | +7.87 | +0.31 ⬆️ |
| FL Entry | 1.1% | 3.2% | +2.1% ⬆️ |
| 学習ステップ | 20M | 9.8M | -10.2M |

---

## 4. 学習停滞の分析

### 4.1 なぜプラトーに達したか

1. **低いExplained Variance (0.15-0.22)**
   ```
   Explained Variance = 価値関数が説明できる分散 / 総分散

   OFCでは:
   - ドロー運による分散が非常に大きい
   - 配置戦略による分散は相対的に小さい
   - → NNは「運のノイズ」と「実力」の区別に苦しむ
   ```

2. **高いスコア分散**
   - 同一ポリシーでも評価間で±5%のファウル率変動
   - これは学習シグナルがノイズに埋もれていることを意味する

3. **報酬シェーピングの限界**
   - Aggressiveの報酬増幅はFL率を改善したが
   - ファウル率の根本的改善には寄与せず

### 4.2 理論的限界

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  OFCの期待値 = f(配置戦略) + ε(ドロー運)                    │
│                                                             │
│  NNが学習できるのは f(配置戦略) の部分のみ                  │
│  ε(ドロー運) は本質的に予測不可能                           │
│                                                             │
│  これ以上の改善には「推論時の探索」が必要                   │
│  → Hybrid Agent (RL + Solver) への移行                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 5. 保存されたモデル

### 5.1 推奨ベースモデル

| バリアント | チェックポイント | ファウル率 | 用途 |
|:---|:---|---:|:---|
| **Self-Play** | `p8_selfplay_5000000.zip` | ~21% | Hybrid Agent基盤 |
| **Aggressive** | `aggressive_1000000.zip` | ~24% | FL重視版 |

### 5.2 全チェックポイント

**Self-Play (ofc-training)**:
```
p8_selfplay_2000000.zip
p8_selfplay_4000000.zip
p8_selfplay_5000000.zip  ← 推奨
p8_selfplay_9800000.zip  ← 最新
```

**Aggressive (ofc-aggressive)**:
```
aggressive_800000.zip
aggressive_1000000.zip
```

---

## 6. 次のフェーズへの提言

### 6.1 Hybrid Agent構築

Pure RLの限界に達したため、推論時の計算力で補完する。

```
Round 1 (初手5枚):   RL Policy Network     → 大局観・定石
Round 2-3 (ドロー):  RL + 軽量MCTS         → リスクバランス
Round 4 (ドロー):    MCTS (探索増加)       → 確定的リスク回避
Round 5 (最終):      完全解析ソルバー       → ミスゼロの確定
```

### 6.2 期待される改善

| コンポーネント | 単体効果 | 累積効果 |
|:---|:---|:---|
| 最良RLモデル | 20.8% | 20.8% |
| + Round 5 Solver | -5% | ~15-16% |
| + MCTS補正 | -2% | ~13-14% |
| **目標** | - | **< 15%** |

---

## 7. 学んだこと

1. **Self-Playは効果的だが限界がある**
   - 過去モデルとの対戦は多様性を提供
   - しかし、ドロー運の分散を克服できない

2. **報酬シェーピングは万能ではない**
   - 特定の行動（FL突入）を促進できる
   - しかし、根本的な戦略品質は変わらない

3. **教師学習は探索を制限する**
   - 初期学習を加速するが
   - 長期的には独自の発見を阻害

4. **OFCには「計算」が必要**
   - 純粋な「直感」（RL）だけでは限界
   - 終盤は計算（Solver）で補完すべき

---

*Document created: 2026-01-18*
