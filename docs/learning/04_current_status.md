# OFC Pineapple AI - 現状説明

## 📊 学習の仕組み（簡単な説明）

```
┌─────────────────────────────────────────────────────────────┐
│                    Self-Play 学習                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Player 0 (学習中)      vs       Player 1 (対戦相手)       │
│   ┌─────────────┐                ┌─────────────┐            │
│   │   PPOモデル  │                │   同じモデル │            │
│   │   (更新中)  │◄──────────────►│   (コピー)  │            │
│   └─────────────┘                └─────────────┘            │
│         │                                                   │
│         ▼                                                   │
│   経験を収集 → 勝敗から学習 → 繰り返し                      │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## ❌ 現在の問題点

### 1. ファウル率が高い（87.5%）
- ほとんどのゲームでファウル（役の強さ違反）している
- 学習がファウル回避を学べていない

### 2. 報酬設計の問題
```python
# 現在の報酬設計
勝ち: +10〜+30点
負け: -10〜-30点
ファウル: ゲーム終了時に負けとして処理

# 問題点
- ファウルのペナルティが即時ではない
- 途中のアクションには報酬がない
- 何が良い/悪いアクションかが学びにくい
```

### 3. 有効アクションマスクの欠如
- 無効なアクションを選んでしまう可能性
- 学習効率が悪い

---

## ✅ 改善案

### 1. ファウル回避の即時報酬
```python
# 改善案
if ファウルになるアクションを選んだ:
    即座に -100 のペナルティ
    
if 役が完成しそう:
    小さな +報酬
```

### 2. 行動マスキング
```python
# 有効なアクションだけを選択可能にする
from sb3_contrib import MaskablePPO

valid_actions = env.get_valid_actions()
action = model.predict(obs, action_masks=valid_actions)
```

### 3. カリキュラム学習
```
Phase 1: まずファウルしないことを学ぶ
Phase 2: 基本的な役作りを学ぶ
Phase 3: 対戦相手を考慮した戦略
```

---

## 📁 関連ファイル

| ファイル | 説明 |
|---------|------|
| `src/python/train_selfplay.py` | Self-Play学習スクリプト |
| `src/python/multi_ofc_env.py` | マルチエージェント環境 |
| `src/python/ofc_env.py` | シングルプレイヤー環境 |
| `src/cpp/game.hpp` | ゲームエンジン（C++） |

---

## 🔧 明日やること

1. [ ] 報酬関数の改善
2. [ ] 有効アクションマスクの実装
3. [ ] ローカルでテスト学習
4. [ ] 改善版をEC2で長時間学習
